<!DOCTYPE html><!--eJmAS4bSgJUF5hZPCAqBJ--><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" as="image" href="/bio.jpg"/><link rel="stylesheet" href="/_next/static/chunks/33fe7063c635829e.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/15f134bf6a9ffc0d.js"/><script src="/_next/static/chunks/41d269a0165a4c3d.js" async=""></script><script src="/_next/static/chunks/3a36804a2acfce4e.js" async=""></script><script src="/_next/static/chunks/e0c1dfd83bab4635.js" async=""></script><script src="/_next/static/chunks/12daa96885968840.js" async=""></script><script src="/_next/static/chunks/turbopack-189a22ee8b659e91.js" async=""></script><script src="/_next/static/chunks/9c176f40581040da.js" async=""></script><script src="/_next/static/chunks/96325a1b41eead81.js" async=""></script><script src="/_next/static/chunks/0137a57162bf5a2a.js" async=""></script><script src="/_next/static/chunks/ff1a16fafef87110.js" async=""></script><script src="/_next/static/chunks/694836347d1e5ef3.js" async=""></script><script src="/_next/static/chunks/6122fa676172875f.js" async=""></script><script src="/_next/static/chunks/82a8bf507689faf5.js" async=""></script><link rel="preload" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap" as="style"/><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><title>Litian Gong</title><meta name="description" content="Master&#x27;s student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."/><meta name="author" content="Litian Gong"/><meta name="keywords" content="Litian Gong,PhD,Research,University of California, Riverside"/><meta name="creator" content="Litian Gong"/><meta name="publisher" content="Litian Gong"/><meta property="og:title" content="Litian Gong"/><meta property="og:description" content="Master&#x27;s student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."/><meta property="og:site_name" content="Litian Gong&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Litian Gong"/><meta name="twitter:description" content="Master&#x27;s student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."/><link rel="icon" href="/favicon.svg"/><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><script src="/_next/static/chunks/a6dad97d9634a72d.js" noModule=""></script></head><body class="font-sans antialiased"><div hidden=""><!--$--><!--/$--></div><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200 focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="/">Litian Gong</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-primary" href="/"><span class="relative z-10">About</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-neutral-600 hover:text-primary" href="/publications/"><span class="relative z-10">Publications</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-neutral-600 hover:text-primary" href="/project/"><span class="relative z-10">Projects</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2 text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-neutral-700/30 bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-neutral-700/30 bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-_R_5pdb_" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen"><div class="grid grid-cols-1 lg:grid-cols-3 gap-12"><div class="lg:col-span-1"><div class="sticky top-8" style="opacity:0;transform:translateY(20px)"><div class="w-64 h-64 mx-auto mb-6 rounded-2xl overflow-hidden shadow-lg hover:shadow-xl transition-all duration-200 hover:scale-105"><img alt="Litian Gong" width="256" height="256" decoding="async" data-nimg="1" class="w-full h-full object-cover object-[32%_center]" style="color:transparent" src="/bio.jpg"/></div><div class="text-center mb-6"><h1 class="text-3xl font-serif font-bold text-primary mb-2">Litian Gong</h1><p class="text-lg text-accent font-medium mb-1">Master&#x27;s Student in EE</p><p class="text-neutral-600 mb-2">University of California, Riverside</p></div><div class="flex flex-wrap justify-center gap-3 sm:gap-4 mb-6 relative px-2"><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Email"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M21.75 6.75v10.5a2.25 2.25 0 0 1-2.25 2.25h-15a2.25 2.25 0 0 1-2.25-2.25V6.75m19.5 0A2.25 2.25 0 0 0 19.5 4.5h-15a2.25 2.25 0 0 0-2.25 2.25m19.5 0v.243a2.25 2.25 0 0 1-1.07 1.916l-7.5 4.615a2.25 2.25 0 0 1-2.36 0L3.32 8.91a2.25 2.25 0 0 1-1.07-1.916V6.75"></path></svg></button></div><div class="relative"><button class="p-2 sm:p-2 transition-colors duration-200 text-neutral-600 dark:text-neutral-400 hover:text-accent" aria-label="Location"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M15 10.5a3 3 0 1 1-6 0 3 3 0 0 1 6 0Z"></path><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 10.5c0 7.142-7.5 11.25-7.5 11.25S4.5 17.642 4.5 10.5a7.5 7.5 0 1 1 15 0Z"></path></svg></button></div><div class="relative inline-block"><a href="https://scholar.google.com/citations?user=cirHYboAAAAJ" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200 inline-block" aria-label="Google Scholar"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5"><path stroke-linecap="round" stroke-linejoin="round" d="M4.26 10.147a60.438 60.438 0 0 0-.491 6.347A48.62 48.62 0 0 1 12 20.904a48.62 48.62 0 0 1 8.232-4.41 60.46 60.46 0 0 0-.491-6.347m-15.482 0a50.636 50.636 0 0 0-2.658-.813A59.906 59.906 0 0 1 12 3.493a59.903 59.903 0 0 1 10.399 5.84c-.896.248-1.783.52-2.658.814m-15.482 0A50.717 50.717 0 0 1 12 13.489a50.702 50.702 0 0 1 7.74-3.342M6.75 15a.75.75 0 1 0 0-1.5.75.75 0 0 0 0 1.5Zm0 0v-3.675A55.378 55.378 0 0 1 12 8.443m-7.007 11.55A5.981 5.981 0 0 0 6.75 15.75v-1.5"></path></svg></a></div><div class="relative inline-block"><a href="https://github.com/gonglitian" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200 inline-block" aria-label="GitHub"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-github h-5 w-5" aria-hidden="true"><path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"></path><path d="M9 18c-4.51 2-5-2-7-2"></path></svg></a></div><div class="relative inline-block"><a href="https://www.linkedin.com/in/Gonglitian/" target="_blank" rel="noopener noreferrer" class="p-2 sm:p-2 text-neutral-600 dark:text-neutral-400 hover:text-accent transition-colors duration-200 inline-block" aria-label="LinkedIn"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-linkedin h-5 w-5" aria-hidden="true"><path d="M16 8a6 6 0 0 1 6 6v7h-4v-7a2 2 0 0 0-2-2 2 2 0 0 0-2 2v7h-4v-7a6 6 0 0 1 6-6z"></path><rect width="4" height="12" x="2" y="9"></rect><circle cx="4" cy="4" r="2"></circle></svg></a></div></div><div class="bg-neutral-100 dark:bg-neutral-800 rounded-lg p-4 mb-6 hover:shadow-lg transition-all duration-200 hover:scale-[1.02]"><h3 class="font-semibold text-primary mb-3">Research Interests</h3><div class="space-y-2 text-sm text-neutral-700 dark:text-neutral-500"><div>Scalable Robot Learning Frameworks</div><div>Imitation and Reinforcement Learning</div><div>Visual-Language Models for Decision Making</div><div>Sim-to-Real Transfer</div><div>Embodied AI</div></div></div><div class="flex justify-center"><div class="relative"><button class="flex items-center space-x-2 px-4 py-2 rounded-lg font-medium text-sm transition-all duration-200 bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-500 hover:bg-red-50 dark:hover:bg-red-900/20 hover:text-red-600 dark:hover:text-red-400 cursor-pointer" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-4 w-4"><path stroke-linecap="round" stroke-linejoin="round" d="M21 8.25c0-2.485-2.099-4.5-4.688-4.5-1.935 0-3.597 1.126-4.312 2.733-.715-1.607-2.377-2.733-4.313-2.733C5.1 3.75 3 5.765 3 8.25c0 7.22 9 12 9 12s9-4.78 9-12Z"></path></svg><span>Like</span></button></div></div></div></div><div class="lg:col-span-2 space-y-8"><section id="about" class="scroll-mt-24 space-y-8"><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">About</h2><div class="text-neutral-700 dark:text-neutral-300 leading-relaxed"><p class="mb-4 last:mb-0">Hi there! I am Litian Gong (龚利天), a Master&#x27;s student in Electrical Engineering at <a href="https://www.ucr.edu/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">University of California - Riverside</a>. I am currently a member of <a href="https://tasl.ucr.edu/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">TASL</a> under the supervision of <a href="https://jiachenli94.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Jiachen Li</a>. I am also a student research intern at the <a href="https://liralab.usc.edu/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Learning and Interactive Robot Autonomy Lab</a> at USC, working with <a href="https://ebiyik.github.io/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Erdem Bıyık</a>. Before that, I obtained my bachelor&#x27;s degree at <a href="https://english.hust.edu.cn/" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Huazhong University of Science and Technology</a> under the supervision of <a href="http://en.seee.hust.edu.cn/info/1057/1336.htm" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm">Prof. Shaorong Wang</a>.</p>
<p class="mb-4 last:mb-0">My research interest lies in robotic learning systems that fuse imitation learning, reinforcement learning, and VLMs to enable multimodal reasoning, long-horizon skills, sim-to-real transfer, and trustworthy, data-efficient autonomy.</p></div></section><section style="opacity:0;transform:translateY(20px)"><div class="flex items-center justify-between mb-4"><h2 class="text-2xl font-serif font-bold text-primary">Selected Publications</h2><a class="text-accent hover:text-accent-dark text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm" href="/publications/">View All →</a></div><div class="space-y-4"><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col sm:flex-row gap-4"><div class="w-full sm:w-32 sm:flex-shrink-0 relative z-0"><div class="aspect-square relative rounded-lg overflow-visible dark:bg-neutral-700 group cursor-pointer"><img alt="AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations" loading="lazy" decoding="async" data-nimg="fill" class="object-contain p-2 transition-all duration-300 ease-in-out group-hover:scale-110 group-hover:z-[100] group-hover:shadow-2xl rounded-lg" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/autofocus-il.png"/></div></div><div class="flex-grow"><h3 class="font-semibold text-primary mb-2 leading-snug">AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><a href="https://gonglitian.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer font-semibold text-accent">Litian Gong</a>, </span><span><a href="https://nfbahrani.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Fatemeh Bahrani</a>, </span><span><a href="https://github.com/yutaizhou" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Yutai Zhou</a>, </span><span><a href="https://aminbana.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Amin Banayeeanzade</a>, </span><span><a href="https://jiachenli94.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Jiachen Li</a>, </span><span><a href="https://ebiyik.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Erdem Bıyık</a></span></p><div class="flex flex-wrap items-center gap-2 mb-3"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-neutral-200 dark:bg-neutral-700 text-neutral-700 dark:text-neutral-300">IEEE International Conference on Robotics and Automation (ICRA)<!-- --> <!-- -->2026</span></div><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2511.18617" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a><a href="https://autofocus-il.github.io/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 21a9.004 9.004 0 0 0 8.716-6.747M12 21a9.004 9.004 0 0 1-8.716-6.747M12 21c2.485 0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485 0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997 0 0 1 7.843 4.582M12 3a8.997 8.997 0 0 0-7.843 4.582m15.686 0A11.953 11.953 0 0 1 12 10.5c-2.998 0-5.74-1.1-7.843-2.918m15.686 0A8.959 8.959 0 0 1 21 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919 0 0 1 12 16.5c-3.162 0-6.133-.815-8.716-2.247m0 0A9.015 9.015 0 0 1 3 12c0-1.605.42-3.113 1.157-4.418"></path></svg>Project</a><a href="https://github.com/autofocus-il/autofocus-il" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M17.25 6.75 22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3-4.5 16.5"></path></svg>Code</a></div></div></div></div><div class="bg-neutral-50 dark:bg-neutral-800 p-4 rounded-lg shadow-sm border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] hover:shadow-lg transition-all duration-200 hover:scale-[1.02]" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col sm:flex-row gap-4"><div class="w-full sm:w-32 sm:flex-shrink-0 relative z-0"><div class="aspect-square relative rounded-lg overflow-visible dark:bg-neutral-700 group cursor-pointer"><img alt="ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models" loading="lazy" decoding="async" data-nimg="fill" class="object-contain p-2 transition-all duration-300 ease-in-out group-hover:scale-110 group-hover:z-[100] group-hover:shadow-2xl rounded-lg" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/oric.png"/></div></div><div class="flex-grow"><h3 class="font-semibold text-primary mb-2 leading-snug">ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</h3><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-1"><span><a href="https://zhaoyangli-1.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Zhaoyang Li</a><sup class="ml-0 text-neutral-600 dark:text-neutral-500">*</sup>, </span><span><a href="https://lz1oceani.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Zhan Ling</a><sup class="ml-0 text-neutral-600 dark:text-neutral-500">*</sup>, </span><span><a href="https://www.yuchenzhou.org/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Yuchen Zhou</a>, </span><span><a href="https://gonglitian.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer font-semibold text-accent">Litian Gong</a>, </span><span><a href="https://ebiyik.github.io/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Erdem Bıyık</a>, </span><span><a href="https://cseweb.ucsd.edu/~haosu/" target="_blank" rel="noopener noreferrer" class="hover:underline cursor-pointer hover:text-accent transition-colors">Hao Su</a></span></p><div class="flex flex-wrap items-center gap-2 mb-3"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-neutral-200 dark:bg-neutral-700 text-neutral-700 dark:text-neutral-300">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)<!-- --> <!-- -->2026</span></div><div class="flex flex-wrap gap-2"><a href="https://arxiv.org/abs/2509.15695" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a><a href="https://github.com/ZhaoyangLi-1/ORIC" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M17.25 6.75 22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3-4.5 16.5"></path></svg>Code</a></div></div></div></div></div></section><section style="opacity:0;transform:translateY(20px)"><h2 class="text-2xl font-serif font-bold text-primary mb-4">News</h2><div class="space-y-3"><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2026-02</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>One paper accepted to CVPR 2026: </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://arxiv.org/abs/2509.15695">ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</a></p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2026-01</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>One paper accepted to ICRA 2026: </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://arxiv.org/abs/2511.18617">AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations</a></p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2025-06</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>Joined the </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://liralab.usc.edu/">Learning and Interactive Robot Autonomy Lab</a><span> at USC, working with </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://ebiyik.github.io/">Prof. Erdem Bıyık</a></p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2024-11</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>Started research at </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://tasl.ucr.edu/">Trustworthy Autonomous Systems Lab</a><span>, UC Riverside, working with </span><a target="_blank" rel="noopener noreferrer" class="text-accent hover:text-accent-dark underline focus-visible:ring-2 focus-visible:ring-accent/50 focus-visible:ring-offset-2" href="https://jiachenli94.github.io/">Prof. Jiachen Li</a></p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2024-09</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>Started Master&#x27;s program in Electrical Engineering at UC Riverside</span></p></div></div><div class="flex items-start space-x-3"><span class="text-xs text-neutral-500 dark:text-neutral-400 mt-1 w-16 flex-shrink-0">2024-06</span><div class="flex-1"><p class="text-sm text-neutral-700 dark:text-neutral-300"><span>Graduated from HUST as Outstanding Undergraduate Graduate</span></p></div></div></div></section><div style="opacity:0;transform:translateY(20px)"><div class="mb-4"><h1 class="text-2xl font-serif font-bold text-primary mb-4">Awards</h1></div><div class="grid gap-4"><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01] cursor-pointer" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="flex-grow"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Outstanding Undergraduate Graduate</h3><span class="text-sm text-neutral-500 font-medium bg-neutral-100 dark:bg-neutral-800 px-2 py-1 rounded">2024</span></div><p class="text-sm text-accent font-medium mb-3">Huazhong University of Science and Technology</p><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed mb-3">Recognized for outstanding academic performance and research contributions.</p></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01] cursor-pointer" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="flex-grow"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Honorable Prize</h3><span class="text-sm text-neutral-500 font-medium bg-neutral-100 dark:bg-neutral-800 px-2 py-1 rounded">2023</span></div><p class="text-sm text-accent font-medium mb-3">Mathematical Contest In Modeling</p><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed mb-3">Achieved honorable mention in the international mathematical modeling competition.</p></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01] cursor-pointer" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="flex-grow"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Second Prize</h3><span class="text-sm text-neutral-500 font-medium bg-neutral-100 dark:bg-neutral-800 px-2 py-1 rounded">2022</span></div><p class="text-sm text-accent font-medium mb-3">China Undergraduate Mathematical Contest in Modeling</p><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed mb-3">Won second prize in the national mathematical modeling competition.</p></div></div></div></div></div><div style="opacity:0;transform:translateY(20px)"><div class="mb-4"><h1 class="text-2xl font-serif font-bold text-primary mb-4">Services</h1></div><div class="grid gap-4"><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01] cursor-pointer" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="flex-grow"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Conference Reviewer</h3><span class="text-sm text-neutral-500 font-medium bg-neutral-100 dark:bg-neutral-800 px-2 py-1 rounded">2025 - present</span></div><p class="text-sm text-accent font-medium mb-3">IEEE International Conference on Robotics and Automation (ICRA)</p><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed mb-3">Reviewing papers for ICRA, focusing on robot learning, imitation learning, and embodied AI.</p></div></div></div><div class="bg-white dark:bg-neutral-900 p-4 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-lg transition-all duration-200 hover:scale-[1.01] cursor-pointer" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-4"><div class="flex-grow"><div class="flex justify-between items-start mb-2"><h3 class="text-lg font-semibold text-primary">Conference Reviewer</h3><span class="text-sm text-neutral-500 font-medium bg-neutral-100 dark:bg-neutral-800 px-2 py-1 rounded">2025 - present</span></div><p class="text-sm text-accent font-medium mb-3">IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p><p class="text-sm text-neutral-600 dark:text-neutral-500 leading-relaxed mb-3">Reviewing papers for CVPR, focusing on vision-language models and computer vision applications in robotics.</p></div></div></div></div></div><div class="" style="opacity:0;transform:translateY(20px)"><h1 class="text-2xl font-serif font-bold text-primary mb-4">CV</h1><div class="text-neutral-700 dark:text-neutral-300 leading-relaxed"><p class="mb-4 last:mb-0"><a href="/Litian_CV.pdf" node="[object Object]" target="_blank" rel="noopener noreferrer" class="text-accent font-medium hover:underline transition-colors focus-visible:outline focus-visible:outline-2 focus-visible:outline-offset-2 focus-visible:outline-accent">Click here for my latest CV in PDF format.</a></p></div></div></section></div></div></div><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->February 21, 2026</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer" class="cursor-pointer transition-colors hover:text-accent">Built with PRISM<svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="lucide lucide-rocket inline ml-1.5 h-3.5 w-3.5" aria-hidden="true"><path d="M4.5 16.5c-1.5 1.26-2 5-2 5s3.74-.5 5-2c.71-.84.7-2.13-.09-2.91a2.18 2.18 0 0 0-2.91-.09z"></path><path d="m12 15-3-3a22 22 0 0 1 2-3.95A12.88 12.88 0 0 1 22 2c0 2.72-.78 7.5-6 11a22.35 22.35 0 0 1-4 2z"></path><path d="M9 12H4s.55-3.03 2-4c1.62-1.08 5 0 5 0"></path><path d="M12 15v5s3.03-.55 4-2c1.08-1.62 0-5 0-5"></path></svg></a></p></div></div></footer></div><script src="/_next/static/chunks/15f134bf6a9ffc0d.js" id="_R_" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[94519,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\"],\"ThemeProvider\"]\n3:I[28127,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\"],\"default\"]\n4:I[39756,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"default\"]\n5:I[37457,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"default\"]\n6:I[58234,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\"],\"default\"]\nc:I[68027,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"default\"]\n:HL[\"/_next/static/chunks/33fe7063c635829e.css\",\"style\"]\n:HL[\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"eJmAS4bSgJUF5hZPCAqBJ\",\"c\":[\"\",\"\"],\"q\":\"\",\"i\":false,\"f\":[[[\"\",{\"children\":[\"__PAGE__\",{}]},\"$undefined\",\"$undefined\",true],[[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/chunks/33fe7063c635829e.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/9c176f40581040da.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/96325a1b41eead81.js\",\"async\":true,\"nonce\":\"$undefined\"}],[\"$\",\"script\",\"script-2\",{\"src\":\"/_next/static/chunks/0137a57162bf5a2a.js\",\"async\":true,\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Projects\",\"type\":\"page\",\"target\":\"project\",\"href\":\"/project\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Litian Gong\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"February 21, 2026\"}]]}]}]]}]]}],{\"children\":[[\"$\",\"$1\",\"c\",{\"children\":[[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen\",\"children\":\"$L7\"}],[\"$L8\",\"$L9\"],\"$La\"]}],{},null,false,false]},null,false,false],\"$Lb\",false]],\"m\":\"$undefined\",\"G\":[\"$c\",[]],\"S\":true}\n"])</script><script>self.__next_f.push([1,"d:I[50058,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\ne:I[90451,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\nf:I[2710,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\n16:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"OutletBoundary\"]\n17:\"$Sreact.suspense\"\n19:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"ViewportBoundary\"]\n1b:I[97367,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"MetadataBoundary\"]\n10:T42b,AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data.11:T50c,Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs."])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"grid grid-cols-1 lg:grid-cols-3 gap-12\",\"children\":[[\"$\",\"div\",null,{\"className\":\"lg:col-span-1\",\"children\":[\"$\",\"$Ld\",null,{\"author\":{\"name\":\"Litian Gong\",\"title\":\"Master's Student in EE\",\"institution\":\"University of California, Riverside\",\"avatar\":\"/bio.jpg\"},\"social\":{\"email\":\"lgong024@ucr.edu\",\"location\":\"Riverside, CA, USA\",\"location_url\":\"https://maps.google.com/?q=Riverside+CA\",\"location_details\":[\"University of California, Riverside\",\"Riverside, CA, USA\"],\"google_scholar\":\"https://scholar.google.com/citations?user=cirHYboAAAAJ\",\"github\":\"https://github.com/gonglitian\",\"linkedin\":\"https://www.linkedin.com/in/Gonglitian/\",\"orcid\":\"\",\"twitter\":\"\",\"youtube\":\"\",\"instagram\":\"\",\"facebook\":\"\",\"researchgate\":\"\"},\"features\":{\"enable_likes\":true,\"enable_one_page_mode\":false},\"researchInterests\":[\"Scalable Robot Learning Frameworks\",\"Imitation and Reinforcement Learning\",\"Visual-Language Models for Decision Making\",\"Sim-to-Real Transfer\",\"Embodied AI\"]}]}],[\"$\",\"div\",null,{\"className\":\"lg:col-span-2 space-y-8\",\"children\":[[\"$\",\"section\",\"about\",{\"id\":\"about\",\"className\":\"scroll-mt-24 space-y-8\",\"children\":[[[\"$\",\"$Le\",\"about\",{\"content\":\"Hi there! I am Litian Gong (龚利天), a Master's student in Electrical Engineering at [University of California - Riverside](https://www.ucr.edu/). I am currently a member of [TASL](https://tasl.ucr.edu/) under the supervision of [Prof. Jiachen Li](https://jiachenli94.github.io/). I am also a student research intern at the [Learning and Interactive Robot Autonomy Lab](https://liralab.usc.edu/) at USC, working with [Prof. Erdem Bıyık](https://ebiyik.github.io/). Before that, I obtained my bachelor's degree at [Huazhong University of Science and Technology](https://english.hust.edu.cn/) under the supervision of [Prof. Shaorong Wang](http://en.seee.hust.edu.cn/info/1057/1336.htm).\\n\\nMy research interest lies in robotic learning systems that fuse imitation learning, reinforcement learning, and VLMs to enable multimodal reasoning, long-horizon skills, sim-to-real transfer, and trustworthy, data-efficient autonomy.\",\"title\":\"About\"}],[\"$\",\"$Lf\",\"featured_publications\",{\"publications\":[{\"id\":\"gong2025autofocusilvlmbasedsaliencymaps\",\"title\":\"AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations\",\"authors\":[{\"name\":\"Litian Gong\",\"url\":\"https://gonglitian.github.io/\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Fatemeh Bahrani\",\"url\":\"https://nfbahrani.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Yutai Zhou\",\"url\":\"https://github.com/yutaizhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Amin Banayeeanzade\",\"url\":\"https://aminbana.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Jiachen Li\",\"url\":\"https://jiachenli94.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Erdem Bıyık\",\"url\":\"https://ebiyik.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false}],\"year\":2026,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Imitation Learning\",\"Visual-Language Models\",\"Saliency Maps\",\"Robot Learning\"],\"keywords\":\"$7:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE International Conference on Robotics and Automation (ICRA)\",\"arxivId\":\"2511.18617\",\"url\":\"https://arxiv.org/abs/2511.18617\",\"code\":\"https://github.com/autofocus-il/autofocus-il\",\"project\":\"https://autofocus-il.github.io/\",\"abstract\":\"AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.\",\"description\":\"$10\",\"selected\":true,\"preview\":\"autofocus-il.png\",\"venue\":\"IEEE International Conference on Robotics and Automation (ICRA)\",\"bibtex\":\"@inproceedings{gong2025autofocusilvlmbasedsaliencymaps,\\n  title = {AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations},\\n  author = {Litian Gong and Fatemeh Bahrani and Yutai Zhou and Amin Banayeeanzade and Jiachen Li and Erdem Bıyık},\\n  booktitle = {IEEE International Conference on Robotics and Automation (ICRA)},\\n  year = {2026},\\n  eprint = {2511.18617},\\n  archivePrefix = {arXiv},\\n  primaryClass = {cs.RO},\\n  url = {https://arxiv.org/abs/2511.18617},\\n  abstract = {AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.}\\n}\"},{\"id\":\"li2025oricbenchmarkingobjectrecognition\",\"title\":\"ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models\",\"authors\":[{\"name\":\"Zhaoyang Li\",\"url\":\"https://zhaoyangli-1.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":true},{\"name\":\"Zhan Ling\",\"url\":\"https://lz1oceani.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":true},{\"name\":\"Yuchen Zhou\",\"url\":\"https://www.yuchenzhou.org/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Litian Gong\",\"url\":\"https://gonglitian.github.io/\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Erdem Bıyık\",\"url\":\"https://ebiyik.github.io/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Hao Su\",\"url\":\"https://cseweb.ucsd.edu/~haosu/\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false}],\"year\":2026,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Vision-Language Models\",\"Object Recognition\",\"Benchmarking\",\"Computer Vision\"],\"keywords\":\"$7:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"arxivId\":\"2509.15695\",\"url\":\"https://arxiv.org/abs/2509.15695\",\"code\":\"https://github.com/ZhaoyangLi-1/ORIC\",\"abstract\":\"We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.\",\"description\":\"$11\",\"selected\":true,\"preview\":\"oric.png\",\"venue\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"bibtex\":\"@inproceedings{li2025oricbenchmarkingobjectrecognition,\\n  title = {ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models},\\n  author = {Zhaoyang Li and Zhan Ling and Yuchen Zhou and Litian Gong and Erdem Bıyık and Hao Su},\\n  booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\\n  year = {2026},\\n  eprint = {2509.15695},\\n  archivePrefix = {},\\n  primaryClass = {cs.CV},\\n  url = {https://arxiv.org/abs/2509.15695},\\n  abstract = {We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.}\\n}\"}],\"title\":\"Selected Publications\",\"enableOnePageMode\":true}],\"$L12\",\"$L13\",\"$L14\",\"$L15\"],false,false,false]}]]}]]}]\n"])</script><script>self.__next_f.push([1,"8:[\"$\",\"script\",\"script-0\",{\"src\":\"/_next/static/chunks/6122fa676172875f.js\",\"async\":true,\"nonce\":\"$undefined\"}]\n9:[\"$\",\"script\",\"script-1\",{\"src\":\"/_next/static/chunks/82a8bf507689faf5.js\",\"async\":true,\"nonce\":\"$undefined\"}]\na:[\"$\",\"$L16\",null,{\"children\":[\"$\",\"$17\",null,{\"name\":\"Next.MetadataOutlet\",\"children\":\"$@18\"}]}]\nb:[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$L19\",null,{\"children\":\"$L1a\"}],[\"$\",\"div\",null,{\"hidden\":true,\"children\":[\"$\",\"$L1b\",null,{\"children\":[\"$\",\"$17\",null,{\"name\":\"Next.Metadata\",\"children\":\"$L1c\"}]}]}],null]}]\n"])</script><script>self.__next_f.push([1,"1d:I[73717,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\n1e:I[36610,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\n1f:I[51205,[\"/_next/static/chunks/9c176f40581040da.js\",\"/_next/static/chunks/96325a1b41eead81.js\",\"/_next/static/chunks/0137a57162bf5a2a.js\",\"/_next/static/chunks/6122fa676172875f.js\",\"/_next/static/chunks/82a8bf507689faf5.js\"],\"default\"]\n12:[\"$\",\"$L1d\",\"news\",{\"items\":[{\"date\":\"2026-02\",\"content\":\"One paper accepted to CVPR 2026: [ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models](https://arxiv.org/abs/2509.15695)\"},{\"date\":\"2026-01\",\"content\":\"One paper accepted to ICRA 2026: [AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations](https://arxiv.org/abs/2511.18617)\"},{\"date\":\"2025-06\",\"content\":\"Joined the [Learning and Interactive Robot Autonomy Lab](https://liralab.usc.edu/) at USC, working with [Prof. Erdem Bıyık](https://ebiyik.github.io/)\"},{\"date\":\"2024-11\",\"content\":\"Started research at [Trustworthy Autonomous Systems Lab](https://tasl.ucr.edu/), UC Riverside, working with [Prof. Jiachen Li](https://jiachenli94.github.io/)\"},{\"date\":\"2024-09\",\"content\":\"Started Master's program in Electrical Engineering at UC Riverside\"},{\"date\":\"2024-06\",\"content\":\"Graduated from HUST as Outstanding Undergraduate Graduate\"}],\"title\":\"News\"}]\n13:[\"$\",\"$L1e\",\"awards\",{\"config\":{\"type\":\"card\",\"title\":\"Awards\",\"items\":[{\"title\":\"Outstanding Undergraduate Graduate\",\"subtitle\":\"Huazhong University of Science and Technology\",\"date\":\"2024\",\"content\":\"Recognized for outstanding academic performance and research contributions.\"},{\"title\":\"Honorable Prize\",\"subtitle\":\"Mathematical Contest In Modeling\",\"date\":\"2023\",\"content\":\"Achieved honorable mention in the international mathematical modeling competition.\"},{\"title\":\"Second Prize\",\"subtitle\":\"China Undergraduate Mathematical Contest in Modeling\",\"date\":\"2022\",\"content\":\"Won second prize in the national mathematical modeling competition.\"}]},\"embedded\":true}]\n14:[\"$\",\"$L1e\",\"services\",{\"config\":{\"type\":\"card\",\"title\":\"Services\",\"items\":[{\"title\":\"Conference Reviewer\",\"subtitle\":\"IEEE International Conference on Robotics and Automation (ICRA)\",\"date\":\"2025 - present\",\"content\":\"Reviewing papers for ICRA, focusing on robot learning, imitation learning, and embodied AI.\"},{\"title\":\"Conference Reviewer\",\"subtitle\":\"IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\",\"date\":\"2025 - present\",\"content\":\"Reviewing papers for CVPR, focusing on vision-language models and computer vision applications in robotics.\"}]},\"embedded\":true}]\n15:[\"$\",\"$L1f\",\"cv\",{\"config\":{\"type\":\"text\",\"title\":\"CV\",\"source\":\"markdown/cv.md\"},\"content\":\"[Click here for my latest CV in PDF format.](/Litian_CV.pdf)\\n\",\"embedded\":true}]\n"])</script><script>self.__next_f.push([1,"1a:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\n"])</script><script>self.__next_f.push([1,"20:I[27201,[\"/_next/static/chunks/ff1a16fafef87110.js\",\"/_next/static/chunks/694836347d1e5ef3.js\"],\"IconMark\"]\n18:null\n1c:[[\"$\",\"title\",\"0\",{\"children\":\"Litian Gong\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Litian Gong,PhD,Research,University of California, Riverside\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Litian Gong's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}],[\"$\",\"$L20\",\"15\",{}]]\n"])</script></body></html>