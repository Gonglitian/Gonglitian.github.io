1:"$Sreact.fragment"
2:I[3719,["161","static/chunks/161-c92285856b08f5b4.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-83a075b4e219aa9a.js","177","static/chunks/app/layout-2e0b700e04730bd2.js"],"ThemeProvider"]
3:I[768,["161","static/chunks/161-c92285856b08f5b4.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-83a075b4e219aa9a.js","177","static/chunks/app/layout-2e0b700e04730bd2.js"],"default"]
4:I[7555,[],""]
5:I[1295,[],""]
6:I[2548,["161","static/chunks/161-c92285856b08f5b4.js","874","static/chunks/874-196dbf0660d69360.js","560","static/chunks/560-83a075b4e219aa9a.js","177","static/chunks/app/layout-2e0b700e04730bd2.js"],"default"]
7:I[2351,["161","static/chunks/161-c92285856b08f5b4.js","299","static/chunks/299-532582fc3fb7f62f.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-5cfa0c6e1546bcb7.js","974","static/chunks/app/page-0df73dd268c60874.js"],"default"]
8:I[9507,["161","static/chunks/161-c92285856b08f5b4.js","299","static/chunks/299-532582fc3fb7f62f.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-5cfa0c6e1546bcb7.js","974","static/chunks/app/page-0df73dd268c60874.js"],"default"]
9:I[5218,["161","static/chunks/161-c92285856b08f5b4.js","299","static/chunks/299-532582fc3fb7f62f.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-5cfa0c6e1546bcb7.js","974","static/chunks/app/page-0df73dd268c60874.js"],"default"]
c:I[1990,["161","static/chunks/161-c92285856b08f5b4.js","299","static/chunks/299-532582fc3fb7f62f.js","874","static/chunks/874-196dbf0660d69360.js","748","static/chunks/748-5cfa0c6e1546bcb7.js","974","static/chunks/app/page-0df73dd268c60874.js"],"default"]
d:I[9665,[],"MetadataBoundary"]
f:I[9665,[],"OutletBoundary"]
12:I[4911,[],"AsyncMetadataOutlet"]
14:I[9665,[],"ViewportBoundary"]
16:I[6614,[],""]
:HL["/_next/static/css/78600482fd656caf.css","style"]
a:T42b,AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data.b:T50c,Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs.0:{"P":null,"b":"4cwngSQR6fMrsPtqBzbpa","p":"","c":["",""],"i":false,"f":[[["",{"children":["__PAGE__",{}]},"$undefined","$undefined",true],["",["$","$1","c",{"children":[[["$","link","0",{"rel":"stylesheet","href":"/_next/static/css/78600482fd656caf.css","precedence":"next","crossOrigin":"$undefined","nonce":"$undefined"}]],["$","html",null,{"lang":"en","className":"scroll-smooth","suppressHydrationWarning":true,"children":[["$","head",null,{"children":[["$","link",null,{"rel":"icon","href":"/favicon.svg","type":"image/svg+xml"}],["$","link",null,{"rel":"dns-prefetch","href":"https://google-fonts.jialeliu.com"}],["$","link",null,{"rel":"preconnect","href":"https://google-fonts.jialeliu.com","crossOrigin":""}],["$","link",null,{"rel":"stylesheet","href":"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&family=Crimson+Text:ital,wght@0,400;0,600;1,400&display=swap"}],["$","script",null,{"dangerouslySetInnerHTML":{"__html":"\n              try {\n                const theme = localStorage.getItem('theme-storage');\n                const parsed = theme ? JSON.parse(theme) : null;\n                const setting = parsed?.state?.theme || 'system';\n                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\n                var root = document.documentElement;\n                root.classList.add(effective);\n                root.setAttribute('data-theme', effective);\n              } catch (e) {\n                var root = document.documentElement;\n                root.classList.add('light');\n                root.setAttribute('data-theme', 'light');\n              }\n            "}}]]}],["$","body",null,{"className":"font-sans antialiased","children":["$","$L2",null,{"children":[["$","$L3",null,{"items":[{"title":"About","type":"page","target":"about","href":"/"},{"title":"Publications","type":"page","target":"publications","href":"/publications"},{"title":"Projects","type":"page","target":"project","href":"/project"},{"title":"Awards","type":"page","target":"awards","href":"/awards"},{"title":"Services","type":"page","target":"services","href":"/services"},{"title":"CV","type":"page","target":"cv","href":"/cv"}],"siteTitle":"Litian Gong","enableOnePageMode":false}],["$","main",null,{"className":"min-h-screen pt-16 lg:pt-20","children":["$","$L4",null,{"parallelRouterKey":"children","error":"$undefined","errorStyles":"$undefined","errorScripts":"$undefined","template":["$","$L5",null,{}],"templateStyles":"$undefined","templateScripts":"$undefined","notFound":[[["$","title",null,{"children":"404: This page could not be found."}],["$","div",null,{"style":{"fontFamily":"system-ui,\"Segoe UI\",Roboto,Helvetica,Arial,sans-serif,\"Apple Color Emoji\",\"Segoe UI Emoji\"","height":"100vh","textAlign":"center","display":"flex","flexDirection":"column","alignItems":"center","justifyContent":"center"},"children":["$","div",null,{"children":[["$","style",null,{"dangerouslySetInnerHTML":{"__html":"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}"}}],["$","h1",null,{"className":"next-error-h1","style":{"display":"inline-block","margin":"0 20px 0 0","padding":"0 23px 0 0","fontSize":24,"fontWeight":500,"verticalAlign":"top","lineHeight":"49px"},"children":404}],["$","div",null,{"style":{"display":"inline-block"},"children":["$","h2",null,{"style":{"fontSize":14,"fontWeight":400,"lineHeight":"49px","margin":0},"children":"This page could not be found."}]}]]}]}]],[]],"forbidden":"$undefined","unauthorized":"$undefined"}]}],["$","$L6",null,{"lastUpdated":"November 27, 2025"}]]}]}]]}]]}],{"children":["__PAGE__",["$","$1","c",{"children":[["$","div",null,{"className":"max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-8 bg-background min-h-screen","children":["$","div",null,{"className":"grid grid-cols-1 lg:grid-cols-3 gap-12","children":[["$","div",null,{"className":"lg:col-span-1","children":["$","$L7",null,{"author":{"name":"Litian Gong","title":"Master's Student in EE","institution":"University of California, Riverside","avatar":"/bio.jpg"},"social":{"email":"lgong024@ucr.edu","location":"Riverside, CA, USA","location_url":"https://maps.google.com/?q=Riverside+CA","location_details":["University of California, Riverside","Riverside, CA, USA"],"google_scholar":"https://scholar.google.com/citations?user=cirHYboAAAAJ","github":"https://github.com/gonglitian","linkedin":"https://www.linkedin.com/in/Gonglitian/","orcid":"","twitter":"","youtube":"","instagram":"","facebook":"","researchgate":""},"features":{"enable_likes":true,"enable_one_page_mode":false},"researchInterests":["Scalable Robot Learning Frameworks","Imitation and Reinforcement Learning","Visual-Language Models for Decision Making","Sim-to-Real Transfer","Embodied AI"]}]}],["$","div",null,{"className":"lg:col-span-2 space-y-8","children":[["$","section","about",{"id":"about","className":"scroll-mt-24 space-y-8","children":[[["$","$L8","about",{"content":"Hi there! I am Litian Gong, a Master's student in Electrical Engineering at [University of California - Riverside](https://www.ucr.edu/). I am currently a member of [TASL](https://tasl.ucr.edu/) under the supervision of [Prof. Jiachen Li](https://jiachenli94.github.io/). I am also a student research intern at the [Learning and Interactive Robot Autonomy Lab](https://liralab.usc.edu/) at USC, working with [Prof. Erdem Bıyık](https://ebiyik.github.io/). Before that, I obtained my bachelor's degree at [Huazhong University of Science and Technology](https://english.hust.edu.cn/).\n\nMy research interest lies in robotic learning systems that fuse imitation learning, reinforcement learning, and VLMs to enable multimodal reasoning, long-horizon skills, sim-to-real transfer, and trustworthy, data-efficient autonomy.","title":"About"}],["$","$L9","featured_publications",{"publications":[{"id":"gong2025autofocusilvlmbasedsaliencymaps","title":"AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations","authors":[{"name":"Litian Gong","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Fatemeh Bahrani","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Yutai Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Amin Banayeeanzade","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Jiachen Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Erdem Bıyık","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false}],"year":2025,"type":"preprint","status":"published","tags":["Imitation Learning","Visual-Language Models","Saliency Maps","Robot Learning"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:0:tags","researchArea":"machine-learning","journal":"","conference":"","arxivId":"2511.18617","url":"https://arxiv.org/abs/2511.18617","code":"https://github.com/autofocus-il/autofocus-il","project":"https://autofocus-il.github.io/","abstract":"We present AutoFocus-IL, a VLM-guided saliency framework that enhances data efficiency and generalization in visual imitation learning without human gaze supervision. The framework implements context-aware object filtering and temporal saliency modeling using Qwen2.5-VL and Grounding DINO to identify and track task-relevant visual cues. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.","description":"$a","selected":true,"preview":"autofocus-il.png","venue":"arXiv","bibtex":"@misc{gong2025autofocusilvlmbasedsaliencymaps,\n  title = {AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations},\n  author = {Litian Gong and Fatemeh Bahrani and Yutai Zhou and Amin Banayeeanzade and Jiachen Li and Erdem Bıyık},\n  year = {2025},\n  eprint = {2511.18617},\n  archivePrefix = {arXiv},\n  primaryClass = {cs.RO},\n  url = {https://arxiv.org/abs/2511.18617},\n  abstract = {We present AutoFocus-IL, a VLM-guided saliency framework that enhances data efficiency and generalization in visual imitation learning without human gaze supervision. The framework implements context-aware object filtering and temporal saliency modeling using Qwen2.5-VL and Grounding DINO to identify and track task-relevant visual cues. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.}\n}"},{"id":"li2025oricbenchmarkingobjectrecognition","title":"ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models","authors":[{"name":"Zhaoyang Li","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":true},{"name":"Zhan Ling","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":true},{"name":"Yuchen Zhou","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Litian Gong","isHighlighted":true,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Erdem Bıyık","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false},{"name":"Hao Su","isHighlighted":false,"isCorresponding":false,"isCoAuthor":false,"isCoFirst":false}],"year":2025,"type":"preprint","status":"published","tags":["Vision-Language Models","Object Recognition","Benchmarking","Computer Vision"],"keywords":"$0:f:0:1:2:children:1:props:children:0:props:children:props:children:1:props:children:0:props:children:0:1:props:publications:1:tags","researchArea":"machine-learning","journal":"","conference":"","arxivId":"2509.15695","url":"https://arxiv.org/abs/2509.15695","code":"https://github.com/ZhaoyangLi-1/ORIC","abstract":"We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.","description":"$b","selected":true,"preview":"oric.png","venue":"arXiv","bibtex":"@misc{li2025oricbenchmarkingobjectrecognition,\n  title = {ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models},\n  author = {Zhaoyang Li and Zhan Ling and Yuchen Zhou and Litian Gong and Erdem Bıyık and Hao Su},\n  year = {2025},\n  eprint = {2509.15695},\n  archivePrefix = {arXiv},\n  primaryClass = {cs.CV},\n  url = {https://arxiv.org/abs/2509.15695},\n  abstract = {We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.}\n}"}],"title":"Selected Publications","enableOnePageMode":true}],["$","$Lc","news",{"items":[{"date":"2025-06","content":"Joined the Learning and [Interactive Robot Autonomy Lab](https://liralab.usc.edu/) at USC, working with [Prof. Erdem Bıyık](https://ebiyik.github.io/)"},{"date":"2024-11","content":"Started research at [Trustworthy Autonomous Systems Lab](https://tasl.ucr.edu/), UC Riverside, working with [Prof. Jiachen Li](https://jiachenli94.github.io/)"},{"date":"2024-09","content":"Started Master's program in Electrical Engineering at UC Riverside"},{"date":"2024-06","content":"Graduated from HUST as Outstanding Undergraduate Graduate"}],"title":"News"}]],false,false,false]}]]}]]}]}],["$","$Ld",null,{"children":"$Le"}],null,["$","$Lf",null,{"children":["$L10","$L11",["$","$L12",null,{"promise":"$@13"}]]}]]}],{},null,false]},null,false],["$","$1","h",{"children":[null,["$","$1","Gx27-QYSOAJd338Xp7jDP",{"children":[["$","$L14",null,{"children":"$L15"}],null]}],null]}],false]],"m":"$undefined","G":["$16","$undefined"],"s":false,"S":true}
17:"$Sreact.suspense"
18:I[4911,[],"AsyncMetadata"]
e:["$","$17",null,{"fallback":null,"children":["$","$L18",null,{"promise":"$@19"}]}]
11:null
15:[["$","meta","0",{"charSet":"utf-8"}],["$","meta","1",{"name":"viewport","content":"width=device-width, initial-scale=1"}]]
10:null
19:{"metadata":[["$","title","0",{"children":"Litian Gong"}],["$","meta","1",{"name":"description","content":"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."}],["$","meta","2",{"name":"author","content":"Litian Gong"}],["$","meta","3",{"name":"keywords","content":"Litian Gong,PhD,Research,University of California, Riverside"}],["$","meta","4",{"name":"creator","content":"Litian Gong"}],["$","meta","5",{"name":"publisher","content":"Litian Gong"}],["$","meta","6",{"property":"og:title","content":"Litian Gong"}],["$","meta","7",{"property":"og:description","content":"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."}],["$","meta","8",{"property":"og:site_name","content":"Litian Gong's Academic Website"}],["$","meta","9",{"property":"og:locale","content":"en_US"}],["$","meta","10",{"property":"og:type","content":"website"}],["$","meta","11",{"name":"twitter:card","content":"summary"}],["$","meta","12",{"name":"twitter:title","content":"Litian Gong"}],["$","meta","13",{"name":"twitter:description","content":"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."}],["$","link","14",{"rel":"icon","href":"/favicon.svg"}]],"error":null,"digest":"$undefined"}
13:{"metadata":"$19:metadata","error":null,"digest":"$undefined"}
