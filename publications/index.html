<!DOCTYPE html><html lang="en" class="scroll-smooth"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="stylesheet" href="/_next/static/css/78600482fd656caf.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-679b75d1c4c2027c.js"/><script src="/_next/static/chunks/4bd1b696-70b6d399998de86a.js" async=""></script><script src="/_next/static/chunks/684-50096e198fb40285.js" async=""></script><script src="/_next/static/chunks/main-app-c2e8e391d77dc171.js" async=""></script><script src="/_next/static/chunks/161-c92285856b08f5b4.js" async=""></script><script src="/_next/static/chunks/874-196dbf0660d69360.js" async=""></script><script src="/_next/static/chunks/560-83a075b4e219aa9a.js" async=""></script><script src="/_next/static/chunks/app/layout-2e0b700e04730bd2.js" async=""></script><script src="/_next/static/chunks/299-532582fc3fb7f62f.js" async=""></script><script src="/_next/static/chunks/748-5cfa0c6e1546bcb7.js" async=""></script><script src="/_next/static/chunks/app/%5Bslug%5D/page-554505d753047240.js" async=""></script><link rel="icon" href="/favicon.svg" type="image/svg+xml"/><link rel="dns-prefetch" href="https://google-fonts.jialeliu.com"/><link rel="preconnect" href="https://google-fonts.jialeliu.com" crossorigin=""/><link rel="stylesheet" href="https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700&amp;family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;display=swap"/><script>
              try {
                const theme = localStorage.getItem('theme-storage');
                const parsed = theme ? JSON.parse(theme) : null;
                const setting = parsed?.state?.theme || 'system';
                const prefersDark = typeof window !== 'undefined' && window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches;
                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));
                var root = document.documentElement;
                root.classList.add(effective);
                root.setAttribute('data-theme', effective);
              } catch (e) {
                var root = document.documentElement;
                root.classList.add('light');
                root.setAttribute('data-theme', 'light');
              }
            </script><title>Publications | Litian Gong</title><meta name="description" content="A collection of my research work."/><meta name="author" content="Litian Gong"/><meta name="keywords" content="Litian Gong,PhD,Research,University of California, Riverside"/><meta name="creator" content="Litian Gong"/><meta name="publisher" content="Litian Gong"/><meta property="og:title" content="Litian Gong"/><meta property="og:description" content="Master&#x27;s student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."/><meta property="og:site_name" content="Litian Gong&#x27;s Academic Website"/><meta property="og:locale" content="en_US"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Litian Gong"/><meta name="twitter:description" content="Master&#x27;s student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning."/><link rel="icon" href="/favicon.svg"/><script>document.querySelectorAll('body link[rel="icon"], body link[rel="apple-touch-icon"]').forEach(el => document.head.appendChild(el))</script><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="font-sans antialiased"><div style="visibility:hidden"><nav class="fixed top-0 left-0 right-0 z-50" data-headlessui-state=""><div class="transition-all duration-300 ease-out bg-transparent" style="transform:translateY(-100px)"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8"><div class="flex justify-between items-center h-16 lg:h-20"><div class="flex-shrink-0" tabindex="0"><a class="text-xl lg:text-2xl font-serif font-semibold text-primary hover:text-accent transition-colors duration-200" href="/">Litian Gong</a></div><div class="hidden lg:block"><div class="ml-10 flex items-center space-x-8"><div class="flex items-baseline space-x-8"><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/"><span class="relative z-10">About</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-primary" href="/publications/"><span class="relative z-10">Publications</span><div class="absolute inset-0 bg-accent/10 rounded-lg"></div></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/project/"><span class="relative z-10">Projects</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/awards/"><span class="relative z-10">Awards</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/services/"><span class="relative z-10">Services</span></a><a class="relative px-3 py-2 text-sm font-medium transition-all duration-200 rounded hover:bg-accent/10 hover:shadow-sm text-neutral-600 hover:text-primary" href="/cv/"><span class="relative z-10">CV</span></a></div><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div></div></div><div class="lg:hidden flex items-center space-x-2"><div class="flex items-center justify-center w-10 h-10 rounded-lg border border-neutral-200 dark:border-[rgba(148,163,184,0.24)] bg-background dark:bg-neutral-800"><div class="w-4 h-4 rounded-full bg-neutral-300 animate-pulse"></div></div><button class="inline-flex items-center justify-center p-2 rounded-md text-neutral-600 hover:text-primary hover:bg-neutral-100 dark:hover:bg-neutral-800 focus:outline-none focus:ring-2 focus:ring-inset focus:ring-accent transition-colors duration-200" id="headlessui-disclosure-button-Â«R5pdbÂ»" type="button" aria-expanded="false" data-headlessui-state=""><span class="sr-only">Open main menu</span><div><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="block h-6 w-6"><path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5"></path></svg></div></button></div></div></div></div></nav><main class="min-h-screen pt-16 lg:pt-20"><div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12"><div style="opacity:0;transform:translateY(20px)"><div class="mb-8"><h1 class="text-4xl font-serif font-bold text-primary mb-4">Publications</h1><p class="text-lg text-neutral-600 dark:text-neutral-500 max-w-2xl">A collection of my research work.</p></div><div class="mb-8 space-y-4"><div class="flex flex-col sm:flex-row gap-4"><div class="relative flex-grow"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="absolute left-3 top-1/2 transform -translate-y-1/2 h-5 w-5 text-neutral-400"><path stroke-linecap="round" stroke-linejoin="round" d="m21 21-5.197-5.197m0 0A7.5 7.5 0 1 0 5.196 5.196a7.5 7.5 0 0 0 10.607 10.607Z"></path></svg><input type="text" placeholder="Search publications..." class="w-full pl-10 pr-4 py-2 rounded-lg border border-neutral-200 dark:border-neutral-800 bg-white dark:bg-neutral-900 focus:ring-2 focus:ring-accent focus:border-transparent transition-all duration-200" value=""/></div><button class="flex items-center justify-center px-4 py-2 rounded-lg border transition-all duration-200 bg-white dark:bg-neutral-900 border-neutral-200 dark:border-neutral-800 text-neutral-600 hover:border-accent hover:text-accent"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-5 w-5 mr-2"><path stroke-linecap="round" stroke-linejoin="round" d="M12 3c2.755 0 5.455.232 8.083.678.533.09.917.556.917 1.096v1.044a2.25 2.25 0 0 1-.659 1.591l-5.432 5.432a2.25 2.25 0 0 0-.659 1.591v2.927a2.25 2.25 0 0 1-1.244 2.013L9.75 21v-6.568a2.25 2.25 0 0 0-.659-1.591L3.659 7.409A2.25 2.25 0 0 1 3 5.818V4.774c0-.54.384-1.006.917-1.096A48.32 48.32 0 0 1 12 3Z"></path></svg>Filters</button></div></div><div class="space-y-6"><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-80 lg:w-96 flex-shrink-0 relative z-0"><div class="aspect-square relative rounded-lg overflow-visible dark:bg-neutral-800 group cursor-pointer"><img alt="AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations" loading="lazy" decoding="async" data-nimg="fill" class="object-contain p-2 transition-all duration-300 ease-in-out group-hover:scale-150 group-hover:z-[100] group-hover:shadow-2xl rounded-lg" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/autofocus-il.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Litian Gong</span>, </span><span><span class="">Fatemeh Bahrani</span>, </span><span><span class="">Yutai Zhou</span>, </span><span><span class="">Amin Banayeeanzade</span>, </span><span><span class="">Jiachen Li</span>, </span><span><span class="">Erdem BÄ±yÄ±k</span></span></p><div class="flex flex-wrap items-center gap-2 mb-3"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-neutral-200 dark:bg-neutral-700 text-neutral-700 dark:text-neutral-300">arXiv<!-- --> <!-- -->2025</span></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4">AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2511.18617" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a><a href="https://autofocus-il.github.io/" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 21a9.004 9.004 0 0 0 8.716-6.747M12 21a9.004 9.004 0 0 1-8.716-6.747M12 21c2.485 0 4.5-4.03 4.5-9S14.485 3 12 3m0 18c-2.485 0-4.5-4.03-4.5-9S9.515 3 12 3m0 0a8.997 8.997 0 0 1 7.843 4.582M12 3a8.997 8.997 0 0 0-7.843 4.582m15.686 0A11.953 11.953 0 0 1 12 10.5c-2.998 0-5.74-1.1-7.843-2.918m15.686 0A8.959 8.959 0 0 1 21 12c0 .778-.099 1.533-.284 2.253m0 0A17.919 17.919 0 0 1 12 16.5c-3.162 0-6.133-.815-8.716-2.247m0 0A9.015 9.015 0 0 1 3 12c0-1.605.42-3.113 1.157-4.418"></path></svg>Project</a><a href="https://github.com/autofocus-il/autofocus-il" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M17.25 6.75 22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3-4.5 16.5"></path></svg>Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-80 lg:w-96 flex-shrink-0 relative z-0"><div class="aspect-square relative rounded-lg overflow-visible dark:bg-neutral-800 group cursor-pointer"><img alt="ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models" loading="lazy" decoding="async" data-nimg="fill" class="object-contain p-2 transition-all duration-300 ease-in-out group-hover:scale-150 group-hover:z-[100] group-hover:shadow-2xl rounded-lg" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/oric.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="">Zhaoyang Li</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">*</sup>, </span><span><span class="">Zhan Ling</span><sup class="ml-0 text-neutral-600 dark:text-neutral-400">*</sup>, </span><span><span class="">Yuchen Zhou</span>, </span><span><span class="font-semibold text-accent">Litian Gong</span>, </span><span><span class="">Erdem BÄ±yÄ±k</span>, </span><span><span class="">Hao Su</span></span></p><div class="flex flex-wrap items-center gap-2 mb-3"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-neutral-200 dark:bg-neutral-700 text-neutral-700 dark:text-neutral-300">arXiv<!-- --> <!-- -->2025</span></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4">Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous object-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://arxiv.org/abs/2509.15695" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M19.5 14.25v-2.625a3.375 3.375 0 0 0-3.375-3.375h-1.5A1.125 1.125 0 0 1 13.5 7.125v-1.5a3.375 3.375 0 0 0-3.375-3.375H8.25m0 12.75h7.5m-7.5 3H12M10.5 2.25H5.625c-.621 0-1.125.504-1.125 1.125v17.25c0 .621.504 1.125 1.125 1.125h12.75c.621 0 1.125-.504 1.125-1.125V11.25a9 9 0 0 0-9-9Z"></path></svg>Paper</a><a href="https://github.com/ZhaoyangLi-1/ORIC" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M17.25 6.75 22.5 12l-5.25 5.25m-10.5 0L1.5 12l5.25-5.25m7.5-3-4.5 16.5"></path></svg>Code</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div><div class="bg-white dark:bg-neutral-900 p-6 rounded-xl shadow-sm border border-neutral-200 dark:border-neutral-800 hover:shadow-md transition-all duration-200" style="opacity:0;transform:translateY(20px)"><div class="flex flex-col md:flex-row gap-6"><div class="w-full md:w-80 lg:w-96 flex-shrink-0 relative z-0"><div class="aspect-square relative rounded-lg overflow-visible dark:bg-neutral-800 group cursor-pointer"><img alt="A Friendly Grid-connected Distribution System with PV and ESS for Remote Rural Residential Family" loading="lazy" decoding="async" data-nimg="fill" class="object-contain p-2 transition-all duration-300 ease-in-out group-hover:scale-150 group-hover:z-[100] group-hover:shadow-2xl rounded-lg" style="position:absolute;height:100%;width:100%;left:0;top:0;right:0;bottom:0;color:transparent" src="/papers/icnepe2023.png"/></div></div><div class="flex-grow"><h3 class="text-xl font-semibold text-primary mb-2 leading-tight">A Friendly Grid-connected Distribution System with PV and ESS for Remote Rural Residential Family</h3><p class="text-base text-neutral-600 dark:text-neutral-400 mb-2"><span><span class="font-semibold text-accent">Litian Gong</span>, </span><span><span class="">Jiaxuan Ren</span>, </span><span><span class="">Shuoyu Jin</span>, </span><span><span class="">Shaorong Wang</span></span></p><div class="flex flex-wrap items-center gap-2 mb-3"><span class="inline-flex items-center px-2.5 py-0.5 rounded-full text-xs font-medium bg-neutral-200 dark:bg-neutral-700 text-neutral-700 dark:text-neutral-300">2023 3rd International Conference on New Energy and Power Engineering (ICNEPE)<!-- --> <!-- -->2023</span></div><p class="text-sm text-neutral-600 dark:text-neutral-500 mb-4">The study of this paper is to utilize distributed PV and energy storage system (ESS) to both extend the capacity and enhance the supply reliability of remote rural residential family distribution system. The proposed new distribution system in this paper has the highlights: The MPPTs of multi groups of PV arrays in parallel in the system are simultaneously implemented by only adjusting in real time the DC bus operation voltage, so that the topology and control of the PV generation subsystem are simplified and reduced on cost. The grid power injects under controlling into the DC bus through the accessing circuit composed of a six-phase diode rectifier with power frequency isolation transformer and a boost circuit, which uses very mature circuit and simple control to reach a higher reliability in a low cost way comparing with the scheme using three-phase fully controllable power electric switches rectifier. Using three single phase DC/AC inverters, each one with power frequency isolation transformer, independent voltage amplitude control loop and coordinated voltage phase control loop, builds the three-phase DC/AC inverter, whose AC port is the type of Y0 wiring three phase four lines, and with the capacity of tolerating power imbalances to a certain degree among phases. In this paper, the topology, functional units, operation modes and their control ways of the proposed distribution system are introduced in detail. Also, the design method and case simulation results of the proposed system are given.</p><div class="flex flex-wrap gap-2 mt-auto"><a href="https://doi.org/10.1109/ICNEPE60694.2023.10429728" target="_blank" rel="noopener noreferrer" class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white transition-colors">DOI</a><button class="inline-flex items-center px-3 py-1 rounded-md text-xs font-medium transition-colors bg-neutral-100 dark:bg-neutral-800 text-neutral-700 dark:text-neutral-300 hover:bg-accent hover:text-white"><svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" aria-hidden="true" data-slot="icon" class="h-3 w-3 mr-1.5"><path stroke-linecap="round" stroke-linejoin="round" d="M12 6.042A8.967 8.967 0 0 0 6 3.75c-1.052 0-2.062.18-3 .512v14.25A8.987 8.987 0 0 1 6 18c2.305 0 4.408.867 6 2.292m0-14.25a8.966 8.966 0 0 1 6-2.292c1.052 0 2.062.18 3 .512v14.25A8.987 8.987 0 0 0 18 18a8.967 8.967 0 0 0-6 2.292m0-14.25v14.25"></path></svg>BibTeX</button></div></div></div></div></div></div></div><!--$--><!--/$--><!--$--><!--/$--></main><footer class="border-t border-neutral-200/50 bg-neutral-50/50 dark:bg-neutral-900/50 dark:border-neutral-700/50"><div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 py-6"><div class="flex flex-col sm:flex-row justify-between items-center gap-2"><p class="text-xs text-neutral-500">Last updated: <!-- -->November 27, 2025</p><p class="text-xs text-neutral-500 flex items-center"><a href="https://github.com/xyjoey/PRISM" target="_blank" rel="noopener noreferrer">Built with PRISM</a><span class="ml-2">ðŸš€</span></p></div></div></footer></div><script src="/_next/static/chunks/webpack-679b75d1c4c2027c.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0])</script><script>self.__next_f.push([1,"1:\"$Sreact.fragment\"\n2:I[3719,[\"161\",\"static/chunks/161-c92285856b08f5b4.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-83a075b4e219aa9a.js\",\"177\",\"static/chunks/app/layout-2e0b700e04730bd2.js\"],\"ThemeProvider\"]\n3:I[768,[\"161\",\"static/chunks/161-c92285856b08f5b4.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-83a075b4e219aa9a.js\",\"177\",\"static/chunks/app/layout-2e0b700e04730bd2.js\"],\"default\"]\n4:I[7555,[],\"\"]\n5:I[1295,[],\"\"]\n6:I[2548,[\"161\",\"static/chunks/161-c92285856b08f5b4.js\",\"874\",\"static/chunks/874-196dbf0660d69360.js\",\"560\",\"static/chunks/560-83a075b4e219aa9a.js\",\"177\",\"static/chunks/app/layout-2e0b700e04730bd2.js\"],\"default\"]\n8:I[9665,[],\"MetadataBoundary\"]\na:I[9665,[],\"OutletBoundary\"]\nd:I[4911,[],\"AsyncMetadataOutlet\"]\nf:I[9665,[],\"ViewportBoundary\"]\n11:I[6614,[],\"\"]\n:HL[\"/_next/static/css/78600482fd656caf.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"0:{\"P\":null,\"b\":\"4cwngSQR6fMrsPtqBzbpa\",\"p\":\"\",\"c\":[\"\",\"publications\",\"\"],\"i\":false,\"f\":[[[\"\",{\"children\":[[\"slug\",\"publications\",\"d\"],{\"children\":[\"__PAGE__\",{}]}]},\"$undefined\",\"$undefined\",true],[\"\",[\"$\",\"$1\",\"c\",{\"children\":[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/78600482fd656caf.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\",\"nonce\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"scroll-smooth\",\"suppressHydrationWarning\":true,\"children\":[[\"$\",\"head\",null,{\"children\":[[\"$\",\"link\",null,{\"rel\":\"icon\",\"href\":\"/favicon.svg\",\"type\":\"image/svg+xml\"}],[\"$\",\"link\",null,{\"rel\":\"dns-prefetch\",\"href\":\"https://google-fonts.jialeliu.com\"}],[\"$\",\"link\",null,{\"rel\":\"preconnect\",\"href\":\"https://google-fonts.jialeliu.com\",\"crossOrigin\":\"\"}],[\"$\",\"link\",null,{\"rel\":\"stylesheet\",\"href\":\"https://google-fonts.jialeliu.com/css2?family=Inter:wght@300;400;500;600;700\u0026family=Crimson+Text:ital,wght@0,400;0,600;1,400\u0026display=swap\"}],[\"$\",\"script\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"\\n              try {\\n                const theme = localStorage.getItem('theme-storage');\\n                const parsed = theme ? JSON.parse(theme) : null;\\n                const setting = parsed?.state?.theme || 'system';\\n                const prefersDark = typeof window !== 'undefined' \u0026\u0026 window.matchMedia \u0026\u0026 window.matchMedia('(prefers-color-scheme: dark)').matches;\\n                const effective = setting === 'dark' ? 'dark' : (setting === 'light' ? 'light' : (prefersDark ? 'dark' : 'light'));\\n                var root = document.documentElement;\\n                root.classList.add(effective);\\n                root.setAttribute('data-theme', effective);\\n              } catch (e) {\\n                var root = document.documentElement;\\n                root.classList.add('light');\\n                root.setAttribute('data-theme', 'light');\\n              }\\n            \"}}]]}],[\"$\",\"body\",null,{\"className\":\"font-sans antialiased\",\"children\":[\"$\",\"$L2\",null,{\"children\":[[\"$\",\"$L3\",null,{\"items\":[{\"title\":\"About\",\"type\":\"page\",\"target\":\"about\",\"href\":\"/\"},{\"title\":\"Publications\",\"type\":\"page\",\"target\":\"publications\",\"href\":\"/publications\"},{\"title\":\"Projects\",\"type\":\"page\",\"target\":\"project\",\"href\":\"/project\"},{\"title\":\"Awards\",\"type\":\"page\",\"target\":\"awards\",\"href\":\"/awards\"},{\"title\":\"Services\",\"type\":\"page\",\"target\":\"services\",\"href\":\"/services\"},{\"title\":\"CV\",\"type\":\"page\",\"target\":\"cv\",\"href\":\"/cv\"}],\"siteTitle\":\"Litian Gong\",\"enableOnePageMode\":false}],[\"$\",\"main\",null,{\"className\":\"min-h-screen pt-16 lg:pt-20\",\"children\":[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":404}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],[]],\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]}],[\"$\",\"$L6\",null,{\"lastUpdated\":\"November 27, 2025\"}]]}]}]]}]]}],{\"children\":[[\"slug\",\"publications\",\"d\"],[\"$\",\"$1\",\"c\",{\"children\":[null,[\"$\",\"$L4\",null,{\"parallelRouterKey\":\"children\",\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L5\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"forbidden\":\"$undefined\",\"unauthorized\":\"$undefined\"}]]}],{\"children\":[\"__PAGE__\",[\"$\",\"$1\",\"c\",{\"children\":[\"$L7\",[\"$\",\"$L8\",null,{\"children\":\"$L9\"}],null,[\"$\",\"$La\",null,{\"children\":[\"$Lb\",\"$Lc\",[\"$\",\"$Ld\",null,{\"promise\":\"$@e\"}]]}]]}],{},null,false]},null,false]},null,false],[\"$\",\"$1\",\"h\",{\"children\":[null,[\"$\",\"$1\",\"QHQloRXcrh7Vco0zj8ahj\",{\"children\":[[\"$\",\"$Lf\",null,{\"children\":\"$L10\"}],null]}],null]}],false]],\"m\":\"$undefined\",\"G\":[\"$11\",\"$undefined\"],\"s\":false,\"S\":true}\n"])</script><script>self.__next_f.push([1,"12:\"$Sreact.suspense\"\n13:I[4911,[],\"AsyncMetadata\"]\n15:I[6669,[\"161\",\"static/chunks/161-c92285856b08f5b4.js\",\"299\",\"static/chunks/299-532582fc3fb7f62f.js\",\"748\",\"static/chunks/748-5cfa0c6e1546bcb7.js\",\"182\",\"static/chunks/app/%5Bslug%5D/page-554505d753047240.js\"],\"default\"]\n9:[\"$\",\"$12\",null,{\"fallback\":null,\"children\":[\"$\",\"$L13\",null,{\"promise\":\"$@14\"}]}]\n16:T42b,AutoFocus-IL is a simple yet effective method to improve data efficiency and generalization in visual imitation learning by guiding policies to attend to task-relevant features rather than distractors and spurious correlations. Although saliency regularization has emerged as a promising way to achieve this, existing approaches typically require costly supervision such as human gaze data or manual saliency annotations. In contrast, AutoFocus-IL leverages vision-language models (VLMs) to automatically identify and track key objects in demonstrations, generating temporal saliency maps that highlight causal visual signals while suppressing distractors. These maps are then used to regularize behavior cloning policies, yielding stronger alignment between visual attention and task-relevant cues. Experiments in both the CARLA simulator and real-robot manipulation tasks demonstrate that AutoFocus-IL not only outperforms standard behavior cloning but also surpasses state-of-the-art baselines that assume privileged access to human supervision, such as gaze data.17:T50c,Large Vision-Language Models (LVLMs) excel at captioning, visual question answering, and robotics by combining vision and language, yet they often miss obvious objects or hallucinate nonexistent ones in atypical scenes. We examine these failures through the lens of uncertainty, focusing on contextual incongruity, where objects appear unexpectedly or fail to appear in expected contexts, and show that such cases increase recognition difficulty for state-of-the-art LVLMs. To study this regime, we introduce the Object Recognition in Incongruous Context (ORIC) framework, which constructs incongruous obj"])</script><script>self.__next_f.push([1,"ect-context pairs through two complementary strategies: (1) LLM-guided sampling to identify hard-to-recognize objects present in the image and (2) CLIP-guided sampling to mine plausible but absent ones. Applied to MSCOCO, ORIC produces ORIC-Bench and ORIC-style training data. Evaluating 18 LVLMs and 2 open-vocabulary detectors reveals substantial performance drops and bias patterns under incongruous contexts. Fine-tuning Qwen3-VL-8B-Instruct with Visual Reinforcement Fine-Tuning on 600 ORIC-style samples improves results on ORIC-Bench, AMBER, and HallusionBench. Overall, we show that contextual incongruity is a key source of uncertainty and provide tools for more reliable LVLMs.18:T5f3,The study of this paper is to utilize distributed PV and energy storage system (ESS) to both extend the capacity and enhance the supply reliability of remote rural residential family distribution system. The proposed new distribution system in this paper has the highlights: The MPPTs of multi groups of PV arrays in parallel in the system are simultaneously implemented by only adjusting in real time the DC bus operation voltage, so that the topology and control of the PV generation subsystem are simplified and reduced on cost. The grid power injects under controlling into the DC bus through the accessing circuit composed of a six-phase diode rectifier with power frequency isolation transformer and a boost circuit, which uses very mature circuit and simple control to reach a higher reliability in a low cost way comparing with the scheme using three-phase fully controllable power electric switches rectifier. Using three single phase DC/AC inverters, each one with power frequency isolation transformer, independent voltage amplitude control loop and coordinated voltage phase control loop, builds the three-phase DC/AC inverter, whose AC port is the type of Y0 wiring three phase four lines, and with the capacity of tolerating power imbalances to a certain degree among phases. In this paper, the topology, functional units, operation modes "])</script><script>self.__next_f.push([1,"and their control ways of the proposed distribution system are introduced in detail. Also, the design method and case simulation results of the proposed system are given."])</script><script>self.__next_f.push([1,"7:[\"$\",\"div\",null,{\"className\":\"max-w-7xl mx-auto px-4 sm:px-6 lg:px-8 py-12\",\"children\":[[\"$\",\"$L15\",null,{\"config\":{\"type\":\"publication\",\"title\":\"Publications\",\"description\":\"A collection of my research work.\",\"source\":\"publications.bib\"},\"publications\":[{\"id\":\"gong2025autofocusilvlmbasedsaliencymaps\",\"title\":\"AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations\",\"authors\":[{\"name\":\"Litian Gong\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Fatemeh Bahrani\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Yutai Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Amin Banayeeanzade\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Jiachen Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Erdem BÄ±yÄ±k\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false}],\"year\":2025,\"type\":\"preprint\",\"status\":\"published\",\"tags\":[\"Imitation Learning\",\"Visual-Language Models\",\"Saliency Maps\",\"Robot Learning\"],\"keywords\":\"$7:props:children:0:props:publications:0:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"\",\"arxivId\":\"2511.18617\",\"url\":\"https://arxiv.org/abs/2511.18617\",\"code\":\"https://github.com/autofocus-il/autofocus-il\",\"project\":\"https://autofocus-il.github.io/\",\"abstract\":\"We present AutoFocus-IL, a VLM-guided saliency framework that enhances data efficiency and generalization in visual imitation learning without human gaze supervision. The framework implements context-aware object filtering and temporal saliency modeling using Qwen2.5-VL and Grounding DINO to identify and track task-relevant visual cues. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.\",\"description\":\"$16\",\"selected\":true,\"preview\":\"autofocus-il.png\",\"venue\":\"arXiv\",\"bibtex\":\"@misc{gong2025autofocusilvlmbasedsaliencymaps,\\n  title = {AutoFocus-IL: VLM-based Saliency Maps for Data-Efficient Visual Imitation Learning without Extra Human Annotations},\\n  author = {Litian Gong and Fatemeh Bahrani and Yutai Zhou and Amin Banayeeanzade and Jiachen Li and Erdem BÄ±yÄ±k},\\n  year = {2025},\\n  eprint = {2511.18617},\\n  archivePrefix = {arXiv},\\n  primaryClass = {cs.RO},\\n  url = {https://arxiv.org/abs/2511.18617},\\n  abstract = {We present AutoFocus-IL, a VLM-guided saliency framework that enhances data efficiency and generalization in visual imitation learning without human gaze supervision. The framework implements context-aware object filtering and temporal saliency modeling using Qwen2.5-VL and Grounding DINO to identify and track task-relevant visual cues. By integrating saliency-guided regularization into behavior cloning, we improve policy robustness in CARLA simulation and real-robot (WidowX) experiments.}\\n}\"},{\"id\":\"li2025oricbenchmarkingobjectrecognition\",\"title\":\"ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models\",\"authors\":[{\"name\":\"Zhaoyang Li\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":true},{\"name\":\"Zhan Ling\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":true},{\"name\":\"Yuchen Zhou\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Litian Gong\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Erdem BÄ±yÄ±k\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Hao Su\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false}],\"year\":2025,\"type\":\"preprint\",\"status\":\"published\",\"tags\":[\"Vision-Language Models\",\"Object Recognition\",\"Benchmarking\",\"Computer Vision\"],\"keywords\":\"$7:props:children:0:props:publications:1:tags\",\"researchArea\":\"machine-learning\",\"journal\":\"\",\"conference\":\"\",\"arxivId\":\"2509.15695\",\"url\":\"https://arxiv.org/abs/2509.15695\",\"code\":\"https://github.com/ZhaoyangLi-1/ORIC\",\"abstract\":\"We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.\",\"description\":\"$17\",\"selected\":true,\"preview\":\"oric.png\",\"venue\":\"arXiv\",\"bibtex\":\"@misc{li2025oricbenchmarkingobjectrecognition,\\n  title = {ORIC: Benchmarking Object Recognition under Contextual Incongruity in Large Vision-Language Models},\\n  author = {Zhaoyang Li and Zhan Ling and Yuchen Zhou and Litian Gong and Erdem BÄ±yÄ±k and Hao Su},\\n  year = {2025},\\n  eprint = {2509.15695},\\n  archivePrefix = {arXiv},\\n  primaryClass = {cs.CV},\\n  url = {https://arxiv.org/abs/2509.15695},\\n  abstract = {We introduce ORIC, a benchmark for evaluating object recognition capabilities of large vision-language models in incongruous contexts. The benchmark tests the robustness and generalization of VLMs when objects appear in unexpected or unusual settings.}\\n}\"},{\"id\":\"10429728\",\"title\":\"A Friendly Grid-connected Distribution System with PV and ESS for Remote Rural Residential Family\",\"authors\":[{\"name\":\"Litian Gong\",\"isHighlighted\":true,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Jiaxuan Ren\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Shuoyu Jin\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false},{\"name\":\"Shaorong Wang\",\"isHighlighted\":false,\"isCorresponding\":false,\"isCoAuthor\":false,\"isCoFirst\":false}],\"year\":2023,\"type\":\"conference\",\"status\":\"published\",\"tags\":[\"Voltage measurement;Simulation;Rectifiers;Transformers;Inverters;Topology;Voltage control;distribution system;remote rural residential family;photovoltaic generation;battery energy storage system;friendly grid-connected characteristic\"],\"keywords\":\"$7:props:children:0:props:publications:2:tags\",\"researchArea\":\"transformer-architectures\",\"journal\":\"\",\"conference\":\"2023 3rd International Conference on New Energy and Power Engineering (ICNEPE)\",\"volume\":\"\",\"issue\":\"\",\"pages\":\"1136-1144\",\"doi\":\"10.1109/ICNEPE60694.2023.10429728\",\"abstract\":\"We developed a friendly grid-connected distribution system integrating photovoltaic (PV) and energy storage systems (ESS) for remote rural residential applications. The system features real-time PV optimization and phase-imbalance-tolerant three-phase inverter control, providing reliable power supply for remote areas.\",\"description\":\"$18\",\"selected\":false,\"preview\":\"icnepe2023.png\",\"venue\":\"2023 3rd International Conference on New Energy and Power Engineering (ICNEPE)\",\"bibtex\":\"@INPROCEEDINGS{10429728,\\n  author = {Gong, Litian and Ren, Jiaxuan and Jin, Shuoyu and Wang, Shaorong},\\n  booktitle = {2023 3rd International Conference on New Energy and Power Engineering (ICNEPE)},\\n  title = {A Friendly Grid-connected Distribution System with PV and ESS for Remote Rural Residential Family},\\n  year = {2023},\\n  volume = {},\\n  number = {},\\n  pages = {1136-1144},\\n  doi = {10.1109/ICNEPE60694.2023.10429728},\\n  abstract = {We developed a friendly grid-connected distribution system integrating photovoltaic (PV) and energy storage systems (ESS) for remote rural residential applications. The system features real-time PV optimization and phase-imbalance-tolerant three-phase inverter control, providing reliable power supply for remote areas.}\\n}\"}]}],false,false]}]\n"])</script><script>self.__next_f.push([1,"c:null\n"])</script><script>self.__next_f.push([1,"10:[[\"$\",\"meta\",\"0\",{\"charSet\":\"utf-8\"}],[\"$\",\"meta\",\"1\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}]]\nb:null\n"])</script><script>self.__next_f.push([1,"14:{\"metadata\":[[\"$\",\"title\",\"0\",{\"children\":\"Publications | Litian Gong\"}],[\"$\",\"meta\",\"1\",{\"name\":\"description\",\"content\":\"A collection of my research work.\"}],[\"$\",\"meta\",\"2\",{\"name\":\"author\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"3\",{\"name\":\"keywords\",\"content\":\"Litian Gong,PhD,Research,University of California, Riverside\"}],[\"$\",\"meta\",\"4\",{\"name\":\"creator\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"5\",{\"name\":\"publisher\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:site_name\",\"content\":\"Litian Gong's Academic Website\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:locale\",\"content\":\"en_US\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"11\",{\"name\":\"twitter:card\",\"content\":\"summary\"}],[\"$\",\"meta\",\"12\",{\"name\":\"twitter:title\",\"content\":\"Litian Gong\"}],[\"$\",\"meta\",\"13\",{\"name\":\"twitter:description\",\"content\":\"Master's student in Electrical Engineering at UC Riverside, researching embodied AI and robot learning.\"}],[\"$\",\"link\",\"14\",{\"rel\":\"icon\",\"href\":\"/favicon.svg\"}]],\"error\":null,\"digest\":\"$undefined\"}\ne:{\"metadata\":\"$14:metadata\",\"error\":null,\"digest\":\"$undefined\"}\n"])</script></body></html>